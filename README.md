# Team12-Problem3

## Setup
1. **Python Environment**
Run install the requirements: 
```python
pip install -r requirement.txt
```
Create a file named account.key containing the Twitter account's username, password, and bearer token.
Ensure to update the path to Google Storage in each file.
2. **Crawled data**
Modify the keywords or users for crawling in the config_keywords.yaml or config_users.yaml file.
Create a folder named data to store the crawled data.
Execute the following commands to start crawling:
- Running the code in tweet_crawler_checkpoint1.py  to start crawling
3. **Data Preprocessing**
- Running the code in data-preprocessing\check_duplicate.py to remove the duplicate records
```
python data-preprocessing\check_duplicate.py
```
- Running the code in data-preprocessing\clean_data.py to keep only potentially relevant fields for each entity
```
python data-preprocessing\clean_data.py
```
- Open the JupyterHub environment to run data-preprocessing\preprocessing_data.py using Spark to process the data before evaluation.
```
python data-preprocessing\preprocessing_data.py
```
4. **Data Analysis**
- Continue in the JupyterHub environment
- Run the write_to_elastic.py code to index the data into Kibana for creating dashboards.   
```
python write_to_elastic.py
```
- After that, implement for analysis in Kibana.
5. **Evaluation**
- Continue in the JupyterHub environment
- Running the code in model_evaluation.py to cluster KOL and project accounts data 
```
python model_evaluation.py
```

## Data Structures
Please visit Data documentation â€“ Group 12.docx

